# ğŸ¤– MOFé¢„æµ‹å¹³å° - ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹é›†æˆè¯´æ˜

## ğŸ“‹ æ¦‚è¿°

MOFé¢„æµ‹å¹³å°ç°å·²å…¨é¢é›†æˆä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ”¯æŒ13ç§ç»å…¸ç®—æ³•ã€å¤šç§é›†æˆç­–ç•¥ã€KæŠ˜äº¤å‰éªŒè¯å’Œè´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–ã€‚

---

## âœ¨ æ–°å¢åŠŸèƒ½

### 1. ğŸŒ² ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹

å¹³å°ç°æ”¯æŒä»¥ä¸‹13ç§ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•ï¼š

#### ğŸŒ³ æ ‘æ¨¡å‹
- **éšæœºæ£®æ— (Random Forest)** - é›†æˆå­¦ä¹ ï¼ŒæŠ—è¿‡æ‹Ÿåˆ
- **æ¢¯åº¦æå‡ (Gradient Boosting)** - sklearnå®ç°
- **XGBoost** - é«˜æ€§èƒ½æ¢¯åº¦æå‡
- **LightGBM** - å¾®è½¯è½»é‡çº§GBM
- **CatBoost** - Yandexç±»åˆ«ç‰¹å¾ä¼˜åŒ–
- **æé™æ ‘ (Extra Trees)** - æ›´å¼ºéšæœºæ€§
- **AdaBoost** - è‡ªé€‚åº”æå‡

#### ğŸ“ çº¿æ€§æ¨¡å‹
- **å²­å›å½’ (Ridge)** - L2æ­£åˆ™åŒ–
- **Lassoå›å½’** - L1æ­£åˆ™åŒ–
- **ElasticNet** - L1+L2æ··åˆæ­£åˆ™åŒ–

#### ğŸ“ å…¶ä»–ç®—æ³•
- **æ”¯æŒå‘é‡å›å½’ (SVR)** - æ ¸æŠ€å·§éçº¿æ€§å›å½’
- **Kè¿‘é‚» (KNN)** - å®ä¾‹å­¦ä¹ 
- **é«˜æ–¯è¿‡ç¨‹ (GP)** - æ¦‚ç‡å›å½’

### 2. ğŸ¯ æ¨¡å‹é›†æˆç­–ç•¥

#### Voting æŠ•ç¥¨é›†æˆ
- å¤šä¸ªæ¨¡å‹é¢„æµ‹ç»“æœå–å¹³å‡
- ç®€å•æœ‰æ•ˆï¼Œé™ä½æ–¹å·®

#### Stacking å †å é›†æˆ
- ä½¿ç”¨å…ƒå­¦ä¹ å™¨ç»„åˆåŸºæ¨¡å‹
- å­¦ä¹ æœ€ä¼˜æƒé‡åˆ†é…

#### Blending æ··åˆé›†æˆ
- æ•°æ®åˆ†å±‚è®­ç»ƒ
- é¿å…ä¿¡æ¯æ³„æ¼

### 3. ğŸ“Š KæŠ˜äº¤å‰éªŒè¯

#### åŠŸèƒ½ç‰¹æ€§
- æ ‡å‡†KæŠ˜åˆ†å‰²
- åˆ†å±‚KæŠ˜ï¼ˆé’ˆå¯¹å›å½’ä»»åŠ¡ï¼‰
- è‡ªåŠ¨è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
- è¯¦ç»†çš„æ¯æŠ˜æ€§èƒ½æŠ¥å‘Š

#### è¯„ä¼°æŒ‡æ ‡
- MAE (Mean Absolute Error)
- RMSE (Root Mean Squared Error)
- RÂ² (R-squared)
- MAPE (Mean Absolute Percentage Error)

### 4. ğŸ” è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–

#### ä¼˜åŒ–ç®—æ³•
- **TPEé‡‡æ ·å™¨** (Tree-structured Parzen Estimator)
- **ä¸­å€¼å‰ªæå™¨** (Median Pruner)
- æ™ºèƒ½æœç´¢ç©ºé—´å®šä¹‰

#### æ”¯æŒçš„è¶…å‚æ•°
æ¯ä¸ªæ¨¡å‹éƒ½æœ‰å®šåˆ¶çš„æœç´¢ç©ºé—´ï¼š
- æ ‘æ·±åº¦ã€å­¦ä¹ ç‡ã€æ­£åˆ™åŒ–å‚æ•°
- æ ·æœ¬é‡‡æ ·ç‡ã€ç‰¹å¾é‡‡æ ·ç‡
- æ¨¡å‹ç‰¹å®šå‚æ•°

### 5. ğŸ“ˆ å¯è§†åŒ–å¢å¼º

#### æ¨¡å‹å¯¹æ¯”å¯è§†åŒ–
- å¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
- KæŠ˜äº¤å‰éªŒè¯ç®±çº¿å›¾
- è®­ç»ƒæ—¶é—´å¯¹æ¯”
- é¢„æµ‹æ•£ç‚¹å›¾å¯¹æ¯”

#### ç‰¹å¾åˆ†æ
- ç‰¹å¾é‡è¦æ€§å¯¹æ¯”
- é›†æˆæ¨¡å‹æƒé‡åˆ†å¸ƒ
- SHAPå€¼åˆ†æ

---

## ğŸš€ ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹

1. **é€‰æ‹©æ¨¡å‹ç±»åˆ«**
   ```
   ğŸ§  æ·±åº¦å­¦ä¹ æ¨¡å‹  |  ğŸŒ² ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹  |  ğŸ¯ é›†æˆæ¨¡å‹
   ```

2. **ä¼ ç»ŸMLæ¨¡å‹è®­ç»ƒæµç¨‹**
   - é€‰æ‹©æ•°æ®æºï¼ˆéœ€è¦æ•°å€¼å‹ç‰¹å¾ï¼‰
   - é€‰æ‹©ç›®æ ‡åˆ—
   - é€‰æ‹©MLç®—æ³•
   - é…ç½®è®­ç»ƒå‚æ•°
   - å¼€å§‹è®­ç»ƒ

3. **KæŠ˜äº¤å‰éªŒè¯**
   ```python
   from models import CrossValidator
   
   cv = CrossValidator(n_splits=5, shuffle=True, random_state=42)
   results = cv.cross_validate(
       model_creator=lambda: TraditionalMLModel('random_forest'),
       X=X_train,
       y=y_train,
       verbose=True
   )
   ```

4. **è¶…å‚æ•°ä¼˜åŒ–**
   ```python
   from models import HyperparameterOptimizer
   
   optimizer = HyperparameterOptimizer(
       model_type='xgboost',
       n_trials=50,
       cv_folds=3
   )
   
   results = optimizer.optimize(X_train, y_train)
   best_params = results['best_params']
   ```

5. **æ¨¡å‹é›†æˆ**
   ```python
   from models import create_auto_ensemble
   
   ensemble = create_auto_ensemble(
       X_train, y_train,
       model_types=['random_forest', 'xgboost', 'lightgbm'],
       ensemble_method='stacking'
   )
   ```

---

## ğŸ“¦ æ–°å¢æ–‡ä»¶ç»“æ„

```
src/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ traditional_ml.py           # ä¼ ç»ŸMLæ¨¡å‹åŒ…è£…å™¨
â”‚   â”œâ”€â”€ ensemble.py                 # é›†æˆç­–ç•¥å®ç°
â”‚   â”œâ”€â”€ cross_validation.py         # KæŠ˜äº¤å‰éªŒè¯
â”‚   â””â”€â”€ hyperparameter_optimization.py  # è´å¶æ–¯ä¼˜åŒ–
â”‚
â””â”€â”€ visualization/
    â””â”€â”€ model_comparison.py         # æ¨¡å‹å¯¹æ¯”å¯è§†åŒ–
```

---

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

### æ¨¡å‹é€‚é…MOFæ•°æ®

ä¼ ç»ŸMLæ¨¡å‹é€šè¿‡ä»¥ä¸‹æ–¹å¼å¤„ç†MOFæ•°æ®ï¼š

1. **è‡ªåŠ¨ç‰¹å¾æå–**
   - ä»DataFrameä¸­è‡ªåŠ¨é€‰æ‹©æ•°å€¼å‹åˆ—
   - æ’é™¤ç›®æ ‡åˆ—
   - å¤„ç†ç¼ºå¤±å€¼

2. **ç‰¹å¾å·¥ç¨‹**
   ```python
   # è‡ªåŠ¨æå–ç‰¹å¾
   numeric_cols = data.select_dtypes(include=[np.number]).columns
   feature_cols = [col for col in numeric_cols if col != target]
   
   X = data[feature_cols].values
   y = data[target].values
   ```

3. **æ•°æ®é¢„å¤„ç†**
   - è‡ªåŠ¨ç¼ºå¤±å€¼å¤„ç†
   - æ ‡å‡†åŒ–ï¼ˆå¯é€‰ï¼‰
   - æ•°æ®åˆ†å‰²

### é›†æˆç­–ç•¥è¯¦è§£

#### Votingï¼ˆæŠ•ç¥¨ï¼‰
```python
prediction = mean([model1.predict(X), model2.predict(X), ...])
```

#### Stackingï¼ˆå †å ï¼‰
```python
# ç¬¬ä¸€å±‚ï¼šåŸºæ¨¡å‹
predictions_layer1 = [model.predict(X) for model in base_models]

# ç¬¬äºŒå±‚ï¼šå…ƒå­¦ä¹ å™¨
meta_features = concat(predictions_layer1)
final_prediction = meta_learner.predict(meta_features)
```

#### Blendingï¼ˆæ··åˆï¼‰
```python
# åˆ†å‰²æ•°æ®
X_base, X_meta = split(X_train)

# åŸºæ¨¡å‹åœ¨X_baseä¸Šè®­ç»ƒ
for model in base_models:
    model.fit(X_base, y_base)
    
# å…ƒå­¦ä¹ å™¨åœ¨X_metaä¸Šè®­ç»ƒ
meta_learner.fit(predictions_on_X_meta, y_meta)
```

### è¶…å‚æ•°æœç´¢ç©ºé—´ç¤ºä¾‹

#### XGBoost
```python
{
    'n_estimators': [50, 500],
    'learning_rate': [0.01, 0.3] (log scale),
    'max_depth': [3, 15],
    'subsample': [0.5, 1.0],
    'colsample_bytree': [0.5, 1.0],
    'gamma': [0, 5],
    'min_child_weight': [1, 10],
    'reg_alpha': [1e-8, 10.0] (log scale),
    'reg_lambda': [1e-8, 10.0] (log scale)
}
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### æ¨¡å‹é€‰æ‹©å»ºè®®

| åœºæ™¯ | æ¨èæ¨¡å‹ | åŸå›  |
|-----|---------|------|
| æ•°æ®é‡å°ï¼ˆ<1000ï¼‰ | Random Forest, GP | æŠ—è¿‡æ‹Ÿåˆ |
| æ•°æ®é‡å¤§ï¼ˆ>10000ï¼‰ | XGBoost, LightGBM | è®­ç»ƒé€Ÿåº¦å¿« |
| éœ€è¦å¯è§£é‡Šæ€§ | Ridge, Lasso, Random Forest | ç‰¹å¾é‡è¦æ€§æ˜ç¡® |
| ç±»åˆ«ç‰¹å¾å¤š | CatBoost | åŸç”Ÿæ”¯æŒç±»åˆ«ç‰¹å¾ |
| è¿½æ±‚æè‡´æ€§èƒ½ | Stacking Ensemble | é›†æˆå¤šç§ç®—æ³• |
| å¿«é€ŸåŸå‹ | Random Forest | é»˜è®¤å‚æ•°æ•ˆæœå¥½ |

### è®­ç»ƒé€Ÿåº¦å¯¹æ¯”ï¼ˆç›¸å¯¹ï¼‰

```
KNN: âš¡âš¡âš¡âš¡âš¡ (æœ€å¿«ï¼Œæ— éœ€è®­ç»ƒ)
Ridge/Lasso: âš¡âš¡âš¡âš¡
Random Forest: âš¡âš¡âš¡
LightGBM: âš¡âš¡âš¡
XGBoost: âš¡âš¡
CatBoost: âš¡âš¡
GradientBoosting: âš¡
Gaussian Process: ğŸŒ (æœ€æ…¢)
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. æ¨¡å‹é€‰æ‹©æµç¨‹

```
å¼€å§‹
  â†“
æ•°æ®æ¢ç´¢ï¼ˆæŸ¥çœ‹æ•°æ®é‡ã€ç‰¹å¾æ•°ã€åˆ†å¸ƒï¼‰
  â†“
å¿«é€ŸåŸºçº¿ï¼ˆRandom Forestï¼‰
  â†“
å°è¯•Boostingæ¨¡å‹ï¼ˆXGBoost/LightGBM/CatBoostï¼‰
  â†“
è¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptunaï¼‰
  â†“
æ¨¡å‹é›†æˆï¼ˆStackingï¼‰
  â†“
KæŠ˜äº¤å‰éªŒè¯è¯„ä¼°
  â†“
æœ€ç»ˆæ¨¡å‹
```

### 2. è¶…å‚æ•°ä¼˜åŒ–å»ºè®®

- **åˆæ­¥æ¢ç´¢**: 20-50æ¬¡è¯•éªŒ
- **ç²¾ç»†è°ƒä¼˜**: 100-200æ¬¡è¯•éªŒ
- **äº¤å‰éªŒè¯**: 3-5æŠ˜ï¼ˆå¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼‰

### 3. é›†æˆç­–ç•¥é€‰æ‹©

- **Voting**: æ¨¡å‹å¤šæ ·æ€§é«˜æ—¶æ•ˆæœå¥½
- **Stacking**: è¿½æ±‚æœ€ä¼˜æ€§èƒ½
- **Blending**: æ•°æ®é‡å¤§æ—¶æ›´å¿«

### 4. é¿å…è¿‡æ‹Ÿåˆ

- ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯
- ç›‘æ§è®­ç»ƒ/éªŒè¯æŸå¤±å·®è·
- åº”ç”¨æ­£åˆ™åŒ–
- å¢åŠ è®­ç»ƒæ•°æ®

---

## ğŸ” è°ƒè¯•ä¸æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¨¡å‹è®­ç»ƒå¤±è´¥**
   - æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«æ•°å€¼å‹ç‰¹å¾
   - ç¡®è®¤ç›®æ ‡åˆ—æ— ç¼ºå¤±å€¼
   - éªŒè¯æ•°æ®é‡æ˜¯å¦å……è¶³ï¼ˆ>10æ¡ï¼‰

2. **æ€§èƒ½ä¸ä½³**
   - å°è¯•ç‰¹å¾å·¥ç¨‹
   - ä½¿ç”¨è¶…å‚æ•°ä¼˜åŒ–
   - å¢åŠ é›†æˆæ¨¡å‹

3. **è®­ç»ƒæ—¶é—´è¿‡é•¿**
   - å‡å°‘n_estimators
   - ä½¿ç”¨LightGBMæ›¿ä»£XGBoost
   - å‡å°‘è¶…å‚æ•°ä¼˜åŒ–è¯•éªŒæ¬¡æ•°

---

## ğŸ“š å‚è€ƒèµ„æº

### åº“æ–‡æ¡£
- [scikit-learn](https://scikit-learn.org/)
- [XGBoost](https://xgboost.readthedocs.io/)
- [LightGBM](https://lightgbm.readthedocs.io/)
- [CatBoost](https://catboost.ai/)
- [Optuna](https://optuna.readthedocs.io/)

### è®ºæ–‡
- Random Forest: Breiman (2001)
- XGBoost: Chen & Guestrin (2016)
- LightGBM: Ke et al. (2017)
- CatBoost: Prokhorenkova et al. (2018)
- Optuna: Akiba et al. (2019)

---

## ğŸ‰ æ€»ç»“

MOFé¢„æµ‹å¹³å°ç°åœ¨æ˜¯ä¸€ä¸ª**çœŸæ­£ä¸“ä¸šå’Œå®Œæ•´çš„æœºå™¨å­¦ä¹ å¹³å°**ï¼š

âœ… **13ç§ç»å…¸MLç®—æ³•**  
âœ… **3ç§é›†æˆç­–ç•¥**  
âœ… **KæŠ˜äº¤å‰éªŒè¯**  
âœ… **è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–**  
âœ… **å®Œæ•´çš„å¯è§†åŒ–åˆ†æ**  
âœ… **æ¨¡å‹æ€§èƒ½å¯¹æ¯”**  
âœ… **ç‰¹å¾é‡è¦æ€§åˆ†æ**  
âœ… **SHAPå¯è§£é‡Šæ€§**  

æ‚¨ç°åœ¨å¯ä»¥ï¼š
- ä½¿ç”¨ä¼ ç»ŸMLç®—æ³•å¿«é€Ÿå»ºç«‹åŸºçº¿
- é€šè¿‡é›†æˆæå‡æ€§èƒ½
- ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§
- ç”¨è´å¶æ–¯ä¼˜åŒ–æ‰¾åˆ°æœ€ä½³è¶…å‚æ•°
- ç”¨ä¸°å¯Œçš„å¯è§†åŒ–åˆ†ææ¨¡å‹è¡Œä¸º

**å¼€å§‹ä½¿ç”¨ä¼ ç»ŸMLæ¨¡å‹å¤„ç†æ‚¨çš„MOFæ•°æ®å§ï¼** ğŸš€

