"""
æ¨¡å‹æ¯”è¾ƒå’Œæ€§èƒ½å¯¹æ¯”å¯è§†åŒ–
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')


def plot_model_comparison(
    results: Dict[str, Dict[str, float]],
    metrics: List[str] = None
) -> go.Figure:
    """
    ç»˜åˆ¶å¤šä¸ªæ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”å›¾
    
    Args:
        results: {model_name: {metric_name: value, ...}, ...}
        metrics: è¦æ˜¾ç¤ºçš„æŒ‡æ ‡åˆ—è¡¨
    
    Returns:
        Plotlyå›¾è¡¨å¯¹è±¡
    """
    if metrics is None:
        # ä»ç¬¬ä¸€ä¸ªæ¨¡å‹è·å–æ‰€æœ‰æŒ‡æ ‡
        first_model = list(results.values())[0]
        metrics = list(first_model.keys())
    
    model_names = list(results.keys())
    n_metrics = len(metrics)
    
    # åˆ›å»ºå­å›¾
    fig = make_subplots(
        rows=1, cols=n_metrics,
        subplot_titles=[m.upper() for m in metrics],
        specs=[[{"type": "bar"}] * n_metrics]
    )
    
    # ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ›å»ºæ¡å½¢å›¾
    for i, metric in enumerate(metrics):
        values = [results[model].get(metric, 0) for model in model_names]
        
        # ç¡®å®šé¢œè‰²ï¼ˆRÂ²è¶Šé«˜è¶Šå¥½ï¼ŒMAE/RMSEè¶Šä½è¶Šå¥½ï¼‰
        if metric.lower() in ['r2', 'rÂ²', 'r_squared']:
            colors = ['green' if v >= 0.8 else 'orange' if v >= 0.6 else 'red' for v in values]
        else:
            colors = ['green' if v <= np.median(values) else 'orange' for v in values]
        
        fig.add_trace(
            go.Bar(
                name=metric,
                x=model_names,
                y=values,
                marker=dict(color=colors),
                text=[f"{v:.4f}" for v in values],
                textposition='outside',
                showlegend=False
            ),
            row=1, col=i+1
        )
    
    fig.update_layout(
        title_text="ğŸ† æ¨¡å‹æ€§èƒ½å¯¹æ¯”",
        height=400,
        showlegend=False
    )
    
    fig.update_xaxes(tickangle=45)
    
    return fig


def plot_cv_results_comparison(
    cv_results: Dict[str, pd.DataFrame]
) -> go.Figure:
    """
    ç»˜åˆ¶äº¤å‰éªŒè¯ç»“æœå¯¹æ¯”
    
    Args:
        cv_results: {model_name: cv_metrics_dataframe, ...}
    
    Returns:
        Plotlyå›¾è¡¨å¯¹è±¡
    """
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=['MAE', 'RMSE', 'RÂ²', 'MAPE'],
        specs=[[{"type": "box"}, {"type": "box"}],
               [{"type": "box"}, {"type": "box"}]]
    )
    
    metrics = ['mae', 'rmse', 'r2', 'mape']
    positions = [(1, 1), (1, 2), (2, 1), (2, 2)]
    
    for metric, (row, col) in zip(metrics, positions):
        for model_name, df in cv_results.items():
            if metric in df.columns:
                fig.add_trace(
                    go.Box(
                        y=df[metric],
                        name=model_name,
                        boxmean='sd',
                        showlegend=(row == 1 and col == 1)
                    ),
                    row=row, col=col
                )
    
    fig.update_layout(
        title_text="ğŸ“Š KæŠ˜äº¤å‰éªŒè¯ç»“æœå¯¹æ¯”",
        height=600,
        showlegend=True
    )
    
    return fig


def plot_training_time_comparison(
    training_times: Dict[str, float]
) -> go.Figure:
    """
    ç»˜åˆ¶è®­ç»ƒæ—¶é—´å¯¹æ¯”
    
    Args:
        training_times: {model_name: training_time_seconds, ...}
    
    Returns:
        Plotlyå›¾è¡¨å¯¹è±¡
    """
    model_names = list(training_times.keys())
    times = list(training_times.values())
    
    # è½¬æ¢ä¸ºæ›´å‹å¥½çš„å•ä½
    if max(times) > 3600:
        times = [t / 3600 for t in times]
        unit = 'å°æ—¶'
    elif max(times) > 60:
        times = [t / 60 for t in times]
        unit = 'åˆ†é’Ÿ'
    else:
        unit = 'ç§’'
    
    # åˆ›å»ºå›¾è¡¨
    fig = go.Figure(data=[
        go.Bar(
            x=model_names,
            y=times,
            text=[f"{t:.2f}{unit}" for t in times],
            textposition='outside',
            marker=dict(
                color=times,
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title=f'æ—¶é—´({unit})')
            )
        )
    ])
    
    fig.update_layout(
        title='â±ï¸ æ¨¡å‹è®­ç»ƒæ—¶é—´å¯¹æ¯”',
        xaxis_title='æ¨¡å‹',
        yaxis_title=f'è®­ç»ƒæ—¶é—´ ({unit})',
        height=400
    )
    
    fig.update_xaxes(tickangle=45)
    
    return fig


def plot_prediction_scatter_comparison(
    predictions: Dict[str, Dict[str, np.ndarray]],
    max_models: int = 4
) -> go.Figure:
    """
    ç»˜åˆ¶å¤šä¸ªæ¨¡å‹çš„é¢„æµ‹æ•£ç‚¹å›¾å¯¹æ¯”
    
    Args:
        predictions: {model_name: {'y_true': [...], 'y_pred': [...]}, ...}
        max_models: æœ€å¤šæ˜¾ç¤ºçš„æ¨¡å‹æ•°
    
    Returns:
        Plotlyå›¾è¡¨å¯¹è±¡
    """
    model_names = list(predictions.keys())[:max_models]
    n_models = len(model_names)
    
    # è®¡ç®—å­å›¾å¸ƒå±€
    n_cols = min(2, n_models)
    n_rows = (n_models + 1) // 2
    
    fig = make_subplots(
        rows=n_rows, cols=n_cols,
        subplot_titles=model_names,
        specs=[[{"type": "scatter"}] * n_cols] * n_rows
    )
    
    for idx, model_name in enumerate(model_names):
        row = idx // n_cols + 1
        col = idx % n_cols + 1
        
        pred_data = predictions[model_name]
        y_true = pred_data['y_true']
        y_pred = pred_data['y_pred']
        
        # è®¡ç®—RÂ²
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
        r2 = 1 - (ss_res / ss_tot)
        
        # é¢„æµ‹å€¼æ•£ç‚¹
        fig.add_trace(
            go.Scatter(
                x=y_true,
                y=y_pred,
                mode='markers',
                name=model_name,
                marker=dict(size=6, opacity=0.6),
                text=[f"çœŸå®: {t:.3f}<br>é¢„æµ‹: {p:.3f}" for t, p in zip(y_true, y_pred)],
                hovertemplate='%{text}<extra></extra>',
                showlegend=False
            ),
            row=row, col=col
        )
        
        # ç†æƒ³çº¿ y=x
        min_val = min(y_true.min(), y_pred.min())
        max_val = max(y_true.max(), y_pred.max())
        fig.add_trace(
            go.Scatter(
                x=[min_val, max_val],
                y=[min_val, max_val],
                mode='lines',
                name='ç†æƒ³çº¿',
                line=dict(color='red', dash='dash', width=2),
                showlegend=(idx == 0)
            ),
            row=row, col=col
        )
        
        # æ·»åŠ RÂ²æ ‡æ³¨
        fig.add_annotation(
            x=0.05, y=0.95,
            xref=f'x{idx+1} domain' if idx > 0 else 'x domain',
            yref=f'y{idx+1} domain' if idx > 0 else 'y domain',
            text=f'RÂ² = {r2:.4f}',
            showarrow=False,
            bgcolor='white',
            bordercolor='black',
            borderwidth=1,
            row=row, col=col
        )
    
    fig.update_layout(
        title_text="ğŸ¯ æ¨¡å‹é¢„æµ‹å¯¹æ¯”",
        height=300 * n_rows,
        showlegend=True
    )
    
    fig.update_xaxes(title_text="çœŸå®å€¼")
    fig.update_yaxes(title_text="é¢„æµ‹å€¼")
    
    return fig


def create_model_ranking_table(
    results: Dict[str, Dict[str, float]],
    metrics: List[str] = None
) -> pd.DataFrame:
    """
    åˆ›å»ºæ¨¡å‹æ’åè¡¨
    
    Args:
        results: {model_name: {metric_name: value, ...}, ...}
        metrics: è¦æ’åçš„æŒ‡æ ‡
    
    Returns:
        DataFrameæ’åè¡¨
    """
    if metrics is None:
        first_model = list(results.values())[0]
        metrics = list(first_model.keys())
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(results).T
    
    # ä¸ºæ¯ä¸ªæŒ‡æ ‡è®¡ç®—æ’å
    ranking = pd.DataFrame(index=df.index)
    
    for metric in metrics:
        if metric in df.columns:
            # RÂ²è¶Šå¤§è¶Šå¥½ï¼Œå…¶ä»–æŒ‡æ ‡è¶Šå°è¶Šå¥½
            if metric.lower() in ['r2', 'rÂ²', 'r_squared']:
                ranking[f'{metric}_rank'] = df[metric].rank(ascending=False)
            else:
                ranking[f'{metric}_rank'] = df[metric].rank(ascending=True)
    
    # è®¡ç®—å¹³å‡æ’å
    rank_columns = [col for col in ranking.columns if col.endswith('_rank')]
    ranking['avg_rank'] = ranking[rank_columns].mean(axis=1)
    ranking['final_rank'] = ranking['avg_rank'].rank()
    
    # åˆå¹¶åŸå§‹æ•°æ®å’Œæ’å
    result_df = pd.concat([df, ranking], axis=1)
    result_df = result_df.sort_values('avg_rank')
    
    return result_df


def plot_ensemble_weights(
    ensemble_model,
    model_names: List[str] = None
) -> go.Figure:
    """
    ç»˜åˆ¶é›†æˆæ¨¡å‹çš„æƒé‡åˆ†å¸ƒ
    
    Args:
        ensemble_model: é›†æˆæ¨¡å‹å®ä¾‹
        model_names: åŸºæ¨¡å‹åç§°åˆ—è¡¨
    
    Returns:
        Plotlyå›¾è¡¨å¯¹è±¡
    """
    # å°è¯•è·å–æ¨¡å‹æƒé‡
    weights = None
    
    if hasattr(ensemble_model, 'weights_'):
        weights = ensemble_model.weights_
    elif hasattr(ensemble_model, 'estimators_'):
        # å¯¹äºVotingRegressorï¼Œé»˜è®¤æƒé‡ç›¸ç­‰
        n_estimators = len(ensemble_model.estimators_)
        weights = np.ones(n_estimators) / n_estimators
    
    if weights is None:
        return None
    
    if model_names is None:
        model_names = [f'Model {i+1}' for i in range(len(weights))]
    
    # åˆ›å»ºé¥¼å›¾
    fig = go.Figure(data=[
        go.Pie(
            labels=model_names,
            values=weights,
            hole=0.3,
            textinfo='label+percent',
            marker=dict(
                colors=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', 
                        '#98D8C8', '#F7DC6F', '#BB8FCE', '#85C1E2']
            )
        )
    ])
    
    fig.update_layout(
        title='ğŸ¯ é›†æˆæ¨¡å‹æƒé‡åˆ†å¸ƒ',
        height=400
    )
    
    return fig


def plot_feature_importance_comparison(
    feature_importances: Dict[str, np.ndarray],
    feature_names: List[str],
    top_k: int = 20
) -> go.Figure:
    """
    ç»˜åˆ¶å¤šä¸ªæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§å¯¹æ¯”
    
    Args:
        feature_importances: {model_name: importance_array, ...}
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        top_k: æ˜¾ç¤ºå‰Kä¸ªé‡è¦ç‰¹å¾
    
    Returns:
        Plotlyå›¾è¡¨å¯¹è±¡
    """
    # è®¡ç®—å¹³å‡é‡è¦æ€§
    all_importances = np.array(list(feature_importances.values()))
    mean_importance = all_importances.mean(axis=0)
    
    # è·å–top-kç‰¹å¾
    top_indices = np.argsort(mean_importance)[-top_k:][::-1]
    top_features = [feature_names[i] for i in top_indices]
    
    # åˆ›å»ºå›¾è¡¨
    fig = go.Figure()
    
    for model_name, importances in feature_importances.items():
        top_importances = importances[top_indices]
        
        fig.add_trace(go.Bar(
            name=model_name,
            x=top_features,
            y=top_importances,
            text=[f"{v:.4f}" for v in top_importances],
            textposition='outside'
        ))
    
    fig.update_layout(
        title=f'ğŸ” ç‰¹å¾é‡è¦æ€§å¯¹æ¯” (Top {top_k})',
        xaxis_title='ç‰¹å¾',
        yaxis_title='é‡è¦æ€§',
        barmode='group',
        height=500,
        xaxis_tickangle=45
    )
    
    return fig

