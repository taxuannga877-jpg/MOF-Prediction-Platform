"""
è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–
ä½¿ç”¨ Optuna è¿›è¡Œæ™ºèƒ½è¶…å‚æ•°æœç´¢
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Callable
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥Optuna
try:
    import optuna
    from optuna.samplers import TPESampler
    from optuna.pruners import MedianPruner
    HAS_OPTUNA = True
except ImportError:
    HAS_OPTUNA = False


class HyperparameterOptimizer:
    """è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–å™¨ï¼ˆåŸºäºOptunaï¼‰"""
    
    def __init__(
        self,
        model_type: str,
        n_trials: int = 50,
        cv_folds: int = 3,
        random_state: int = 42,
        direction: str = 'maximize'
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        Args:
            model_type: æ¨¡å‹ç±»å‹
            n_trials: ä¼˜åŒ–è¯•éªŒæ¬¡æ•°
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­
            direction: ä¼˜åŒ–æ–¹å‘ ('maximize' æˆ– 'minimize')
        """
        if not HAS_OPTUNA:
            raise ImportError("è¯·å…ˆå®‰è£… Optuna: pip install optuna")
        
        self.model_type = model_type
        self.n_trials = n_trials
        self.cv_folds = cv_folds
        self.random_state = random_state
        self.direction = direction
        
        self.study = None
        self.best_params = None
        self.optimization_history = []
    
    def _get_search_space(self, trial: 'optuna.Trial') -> Dict[str, Any]:
        """
        å®šä¹‰å„æ¨¡å‹çš„è¶…å‚æ•°æœç´¢ç©ºé—´
        
        Args:
            trial: Optunaè¯•éªŒå¯¹è±¡
        
        Returns:
            è¶…å‚æ•°å­—å…¸
        """
        if self.model_type == 'random_forest':
            return {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'max_depth': trial.suggest_int('max_depth', 3, 30),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
                'random_state': self.random_state
            }
        
        elif self.model_type == 'gradient_boosting':
            return {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'max_depth': trial.suggest_int('max_depth', 2, 10),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
                'random_state': self.random_state
            }
        
        elif self.model_type == 'xgboost':
            return {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'max_depth': trial.suggest_int('max_depth', 3, 15),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                'gamma': trial.suggest_float('gamma', 0, 5),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
                'random_state': self.random_state
            }
        
        elif self.model_type == 'lightgbm':
            return {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'num_leaves': trial.suggest_int('num_leaves', 20, 150),
                'max_depth': trial.suggest_int('max_depth', 3, 15),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
                'random_state': self.random_state,
                'verbose': -1
            }
        
        elif self.model_type == 'catboost':
            return {
                'iterations': trial.suggest_int('iterations', 50, 500),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'depth': trial.suggest_int('depth', 3, 10),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),
                'random_state': self.random_state,
                'verbose': False
            }
        
        elif self.model_type == 'ridge':
            return {
                'alpha': trial.suggest_float('alpha', 1e-3, 100.0, log=True),
                'random_state': self.random_state
            }
        
        elif self.model_type == 'lasso':
            return {
                'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),
                'random_state': self.random_state
            }
        
        elif self.model_type == 'elasticnet':
            return {
                'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),
                'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 1.0),
                'random_state': self.random_state
            }
        
        elif self.model_type == 'svr':
            return {
                'kernel': trial.suggest_categorical('kernel', ['rbf', 'poly', 'sigmoid']),
                'C': trial.suggest_float('C', 0.1, 100.0, log=True),
                'epsilon': trial.suggest_float('epsilon', 0.01, 1.0, log=True),
                'gamma': trial.suggest_categorical('gamma', ['scale', 'auto'])
            }
        
        elif self.model_type == 'knn':
            return {
                'n_neighbors': trial.suggest_int('n_neighbors', 3, 20),
                'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),
                'p': trial.suggest_int('p', 1, 2)
            }
        
        else:
            return {}
    
    def optimize(
        self,
        X: np.ndarray,
        y: np.ndarray,
        model_creator: Callable,
        metric: str = 'r2',
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: æ ‡ç­¾æ•°æ®
            model_creator: æ¨¡å‹åˆ›å»ºå‡½æ•°
            metric: è¯„ä¼°æŒ‡æ ‡ ('r2', 'mae', 'rmse')
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        from sklearn.model_selection import cross_val_score
        from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error
        
        # å®šä¹‰ç›®æ ‡å‡½æ•°
        def objective(trial):
            # è·å–è¶…å‚æ•°
            params = self._get_search_space(trial)
            
            # åˆ›å»ºæ¨¡å‹
            model = model_creator(**params)
            
            # å¦‚æœæ˜¯è‡ªå®šä¹‰æ¨¡å‹æ¥å£ï¼Œéœ€è¦è½¬æ¢
            if hasattr(model, 'model'):
                model = model.model
            
            # äº¤å‰éªŒè¯è¯„ä¼°
            if metric == 'r2':
                scores = cross_val_score(
                    model, X, y,
                    cv=self.cv_folds,
                    scoring='r2',
                    n_jobs=-1
                )
            elif metric == 'mae':
                scorer = make_scorer(mean_absolute_error, greater_is_better=False)
                scores = cross_val_score(
                    model, X, y,
                    cv=self.cv_folds,
                    scoring=scorer,
                    n_jobs=-1
                )
            elif metric == 'rmse':
                scorer = make_scorer(
                    lambda y_true, y_pred: -np.sqrt(mean_squared_error(y_true, y_pred)),
                    greater_is_better=False
                )
                scores = cross_val_score(
                    model, X, y,
                    cv=self.cv_folds,
                    scoring=scorer,
                    n_jobs=-1
                )
            
            mean_score = scores.mean()
            
            # è®°å½•å†å²
            self.optimization_history.append({
                'trial': trial.number,
                'params': params,
                'score': mean_score
            })
            
            return mean_score
        
        # åˆ›å»ºç ”ç©¶
        if verbose:
            print(f"ğŸš€ å¼€å§‹è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–")
            print(f"   æ¨¡å‹ç±»å‹: {self.model_type}")
            print(f"   è¯•éªŒæ¬¡æ•°: {self.n_trials}")
            print(f"   äº¤å‰éªŒè¯æŠ˜æ•°: {self.cv_folds}")
            print(f"   ä¼˜åŒ–æŒ‡æ ‡: {metric}")
            print(f"{'='*60}\n")
        
        self.study = optuna.create_study(
            direction=self.direction,
            sampler=TPESampler(seed=self.random_state),
            pruner=MedianPruner()
        )
        
        # æ‰§è¡Œä¼˜åŒ–
        self.study.optimize(
            objective,
            n_trials=self.n_trials,
            show_progress_bar=verbose
        )
        
        # è·å–æœ€ä½³å‚æ•°
        self.best_params = self.study.best_params
        
        if verbose:
            print(f"\n{'='*60}")
            print(f"âœ… ä¼˜åŒ–å®Œæˆï¼")
            print(f"   æœ€ä½³åˆ†æ•°: {self.study.best_value:.4f}")
            print(f"   æœ€ä½³å‚æ•°:")
            for param, value in self.best_params.items():
                print(f"      {param}: {value}")
            print(f"{'='*60}\n")
        
        return {
            'best_params': self.best_params,
            'best_score': self.study.best_value,
            'n_trials': len(self.study.trials),
            'optimization_history': self.optimization_history
        }
    
    def get_optimization_history(self) -> pd.DataFrame:
        """è·å–ä¼˜åŒ–å†å²DataFrame"""
        return pd.DataFrame(self.optimization_history)
    
    def plot_optimization_history(self):
        """ç»˜åˆ¶ä¼˜åŒ–å†å²"""
        if not HAS_OPTUNA or self.study is None:
            return None
        
        import plotly.graph_objects as go
        
        # è·å–å†å²æ•°æ®
        trials = self.study.trials
        trial_numbers = [t.number for t in trials]
        scores = [t.value for t in trials]
        
        # è®¡ç®—ç´¯ç§¯æœ€ä¼˜å€¼
        if self.direction == 'maximize':
            best_scores = [max(scores[:i+1]) for i in range(len(scores))]
        else:
            best_scores = [min(scores[:i+1]) for i in range(len(scores))]
        
        # åˆ›å»ºå›¾è¡¨
        fig = go.Figure()
        
        # æ‰€æœ‰è¯•éªŒçš„åˆ†æ•°
        fig.add_trace(go.Scatter(
            x=trial_numbers,
            y=scores,
            mode='markers',
            name='è¯•éªŒåˆ†æ•°',
            marker=dict(size=8, color='lightblue', opacity=0.6)
        ))
        
        # ç´¯ç§¯æœ€ä¼˜åˆ†æ•°
        fig.add_trace(go.Scatter(
            x=trial_numbers,
            y=best_scores,
            mode='lines+markers',
            name='æœ€ä¼˜åˆ†æ•°',
            line=dict(color='red', width=2),
            marker=dict(size=6)
        ))
        
        fig.update_layout(
            title='è¶…å‚æ•°ä¼˜åŒ–å†å²',
            xaxis_title='è¯•éªŒç¼–å·',
            yaxis_title='è¯„åˆ†',
            hovermode='x unified',
            height=400
        )
        
        return fig
    
    def plot_param_importances(self):
        """ç»˜åˆ¶å‚æ•°é‡è¦æ€§"""
        if not HAS_OPTUNA or self.study is None:
            return None
        
        import plotly.graph_objects as go
        
        # è®¡ç®—å‚æ•°é‡è¦æ€§
        importance = optuna.importance.get_param_importances(self.study)
        
        params = list(importance.keys())
        values = list(importance.values())
        
        # åˆ›å»ºå›¾è¡¨
        fig = go.Figure(go.Bar(
            x=values,
            y=params,
            orientation='h',
            marker=dict(
                color=values,
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title='é‡è¦æ€§')
            )
        ))
        
        fig.update_layout(
            title='è¶…å‚æ•°é‡è¦æ€§',
            xaxis_title='é‡è¦æ€§åˆ†æ•°',
            yaxis_title='å‚æ•°',
            height=max(300, len(params) * 30)
        )
        
        return fig


def quick_optimize(
    model_type: str,
    X: np.ndarray,
    y: np.ndarray,
    n_trials: int = 50,
    cv_folds: int = 3,
    metric: str = 'r2',
    verbose: bool = True
) -> Tuple[Dict[str, Any], Any]:
    """
    å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–
    
    Args:
        model_type: æ¨¡å‹ç±»å‹
        X: ç‰¹å¾
        y: æ ‡ç­¾
        n_trials: è¯•éªŒæ¬¡æ•°
        cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
        metric: è¯„ä¼°æŒ‡æ ‡
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        (æœ€ä½³å‚æ•°, ä¼˜åŒ–å™¨å®ä¾‹)
    """
    from .traditional_ml import TraditionalMLModel
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = HyperparameterOptimizer(
        model_type=model_type,
        n_trials=n_trials,
        cv_folds=cv_folds,
        direction='maximize' if metric == 'r2' else 'minimize'
    )
    
    # æ‰§è¡Œä¼˜åŒ–
    def model_creator(**params):
        return TraditionalMLModel(model_type=model_type, **params)
    
    results = optimizer.optimize(X, y, model_creator, metric, verbose)
    
    return results['best_params'], optimizer
